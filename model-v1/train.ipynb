{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: pyparsing>=3.0.9 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydot) (3.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\simpl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pydot\n",
    "%pip install tensorflow\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from constants import (\n",
    "    DATA_INPUT_PATH,\n",
    "    CLASSES,\n",
    "    MODEL_PATH,\n",
    "    METADATA_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the files in the data dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('data/7_9_dylang7.1.csv'), WindowsPath('data/7_9_dylang7.csv')]\n"
     ]
    }
   ],
   "source": [
    "# Read all of the files in the data folder\n",
    "files_in_folder = Path(DATA_INPUT_PATH).rglob(\"*.csv\")\n",
    "\n",
    "files = [x for x in files_in_folder]\n",
    "print([file for file in files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert .csv(s) to dataframes and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sensor1  Sensor2  Sensor3  Sensor4  Sensor5  Sensor6  Sensor7  Sensor8  \\\n",
      "0        0       -6      -20       10        5        1        2        0   \n",
      "1        2        0       25        8       -1       -1        2        2   \n",
      "2       -1        6        4       -1        0        1        0       -1   \n",
      "3        2       19       17        0        6        4        1        0   \n",
      "4        0      -27      -16       -5       -1       -3       -1        0   \n",
      "\n",
      "   Sensor9  Sensor10  Sensor11  Sensor12  Sensor13  Sensor14  Sensor15  \\\n",
      "0       -2        11         9        -4        -4        -2         0   \n",
      "1        0        -5         3        -2        -4        -5        -4   \n",
      "2        0         3         2        -3         0         0        -2   \n",
      "3        5       -14        26        26        13         3         3   \n",
      "4       -5        38        30        -7        -4        -4        -3   \n",
      "\n",
      "   Sensor16  Label  \n",
      "0        -1      0  \n",
      "1        -1      0  \n",
      "2        -2      0  \n",
      "3         3      0  \n",
      "4        -5      0  \n",
      "Shape of dataframe before removing duplicates (135658, 17)\n",
      "Shape of dataframe after removing duplicates (135658, 17)\n",
      "(135658, 17)\n"
     ]
    }
   ],
   "source": [
    "# Read the data from the files\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(str(file))\n",
    "    dfs.append(df)\n",
    "        \n",
    "# Convert the data to a DataFrame\n",
    "df = pd.concat([x for x in dfs], axis=0)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Before removing duplicates\n",
    "print(f\"Shape of dataframe before removing duplicates {df.shape}\")\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Shape of dataframe after removing duplicates {df.shape}\")\n",
    "\n",
    "# Parameters\n",
    "window_size = 30  # Number of past samples to include\n",
    "\n",
    "# Ensure the DataFrame is sorted by time or index if necessary\n",
    "# df = df.sort_values(by='time_column')  # Uncomment if there's a time column\n",
    "\n",
    "# Extract feature columns (excluding label, if it exists)\n",
    "feature_columns = [col for col in df.columns if col != 'Label']\n",
    "features = df[feature_columns].to_numpy()\n",
    "\n",
    "# Convert features into a 3D array: [samples, features, time]\n",
    "n_samples = len(features)\n",
    "n_features = len(feature_columns)\n",
    "reshaped_data = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Get the last `window_size` rows or fewer for each sample\n",
    "    window = features[max(i - window_size + 1, 0):i + 1]\n",
    "    \n",
    "    # Pad the window if it has fewer than `window_size` rows\n",
    "    if len(window) < window_size:\n",
    "        padding = np.zeros((window_size - len(window), n_features))\n",
    "        window = np.vstack((padding, window))\n",
    "    \n",
    "    reshaped_data.append(window)\n",
    "\n",
    "# Convert list to 3D numpy array\n",
    "reshaped_data = np.stack(reshaped_data, axis=0)\n",
    "reshaped_data = reshaped_data.transpose(0, 2, 1)\n",
    "df[feature_columns] = np.sqrt(np.mean(np.square(reshaped_data), axis=2))\n",
    "    \n",
    "print(df.shape)\n",
    "# remove rows where the label is 0 or 1\n",
    "# df = df[df[\"Label\"] != 0] # unmarked\n",
    "# df = df[df[\"Label\"] != 1] # rest\n",
    "\n",
    "# Print out the columns\n",
    "# classes = {\n",
    "#     \"0\": \"unmarked\",\n",
    "#     \"1\": \"rest\",\n",
    "#     \"2\": \"clenched_fist\",\n",
    "#     \"3\": \"wrist_flexion\",\n",
    "#     \"4\": \"wrist_extension\",\n",
    "#     \"5\": \"radial_deviations\",\n",
    "#     \"6\": \"ulnar_deviations\",\n",
    "#     \"7\": \"extended_palm\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_counts = df['Label'].value_counts().sort_index()\n",
    "\n",
    "# plt.figure(figsize=(15, 8))\n",
    "# bars = plt.bar(range(len(class_counts)), class_counts.values)\n",
    "\n",
    "# plt.title('EMG Signal Class Distribution', fontsize=14, pad=20)\n",
    "# plt.xlabel('Class', fontsize=12)\n",
    "# plt.ylabel('Number of Samples', fontsize=12)\n",
    "\n",
    "# # Set x-axis ticks with class labels\n",
    "# plt.xticks(range(len(class_counts)), [CLASSES[i] for i in range(len(class_counts))], rotation=45, ha='right')\n",
    "\n",
    "# # Add value labels on top of each bar\n",
    "# for bar in bars:\n",
    "#     height = bar.get_height()\n",
    "#     plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "#              f'{int(height):,}',\n",
    "#              ha='center', va='bottom')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group data by label and calculate mean for each sensor\n",
    "# sensor_avgs = []\n",
    "# sensor_stds = []\n",
    "\n",
    "# for label in range(8):\n",
    "#     label_data = df[df['Label'] == label].drop('Label', axis=1)\n",
    "#     sensor_avgs.append(label_data.mean().values)\n",
    "#     sensor_stds.append(label_data.std().values)\n",
    "\n",
    "# sensor_avgs = np.array(sensor_avgs)\n",
    "# sensor_stds = np.array(sensor_stds)\n",
    "\n",
    "# # Set up the plot\n",
    "# plt.figure(figsize=(15, 8))\n",
    "\n",
    "# # Plot bars for each gesture\n",
    "# x = np.arange(16)\n",
    "# width = 0.1\n",
    "# for i in range(4):\n",
    "#     plt.bar(x + i*width, sensor_avgs[i], width, \n",
    "#             label=CLASSES[i],\n",
    "#             yerr=sensor_stds[i],\n",
    "#             capsize=2)\n",
    "\n",
    "# # Customize the plot\n",
    "# plt.xlabel('Sensor Number')\n",
    "# plt.ylabel('Average Reading')\n",
    "# plt.title('Average EMG Sensor Readings by Gesture')\n",
    "# plt.xticks(x + width*4, [f'S{i+1}' for i in range(16)])\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the subplot grid for all 16 sensors x 8 classes\n",
    "# fig, axs = plt.subplots(8, 16, figsize=(30, 20))\n",
    "# fig.suptitle('EMG Sensor Data Distribution by Class and Sensor')\n",
    "\n",
    "# bins = np.arange(-128, 129, 16) # Create bins from -128 to 128 in steps of 16\n",
    "\n",
    "# for class_id, class_name in CLASSES.items():\n",
    "#    class_data = df[df['Label'] == class_id]\n",
    "   \n",
    "#    for sensor in range(16):\n",
    "#        sensor_col = f'Sensor{sensor+1}'\n",
    "#        sensor_data = class_data[sensor_col]\n",
    "       \n",
    "#        axs[class_id, sensor].hist(sensor_data, bins=bins)\n",
    "#        axs[class_id, sensor].set_title(f'{CLASSES[class_id]} - {sensor_col}')\n",
    "       \n",
    "#        # Only show y-axis labels for leftmost plots\n",
    "#        if sensor != 0:\n",
    "#            axs[class_id, sensor].set_yticklabels([])\n",
    "           \n",
    "#        # Only show x-axis labels for bottom plots\n",
    "#        if class_id != 7:\n",
    "#            axs[class_id, sensor].set_xticklabels([])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale and clean the data, then train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simpl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.8074 - loss: 0.6121 - val_accuracy: 0.9312 - val_loss: 0.2163\n",
      "Epoch 2/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9126 - loss: 0.2791 - val_accuracy: 0.9323 - val_loss: 0.1972\n",
      "Epoch 3/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9254 - loss: 0.2310 - val_accuracy: 0.9456 - val_loss: 0.1625\n",
      "Epoch 4/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9305 - loss: 0.2078 - val_accuracy: 0.9529 - val_loss: 0.1390\n",
      "Epoch 5/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9383 - loss: 0.1877 - val_accuracy: 0.9556 - val_loss: 0.1296\n",
      "Epoch 6/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9408 - loss: 0.1711 - val_accuracy: 0.9580 - val_loss: 0.1178\n",
      "Epoch 7/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9467 - loss: 0.1582 - val_accuracy: 0.9549 - val_loss: 0.1236\n",
      "Epoch 8/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9484 - loss: 0.1495 - val_accuracy: 0.9658 - val_loss: 0.1000\n",
      "Epoch 9/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9514 - loss: 0.1405 - val_accuracy: 0.9616 - val_loss: 0.1061\n",
      "Epoch 10/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9533 - loss: 0.1355 - val_accuracy: 0.9667 - val_loss: 0.0904\n",
      "Epoch 11/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9545 - loss: 0.1291 - val_accuracy: 0.9672 - val_loss: 0.0945\n",
      "Epoch 12/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9567 - loss: 0.1217 - val_accuracy: 0.9712 - val_loss: 0.0801\n",
      "Epoch 13/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9600 - loss: 0.1152 - val_accuracy: 0.9654 - val_loss: 0.0963\n",
      "Epoch 14/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9590 - loss: 0.1145 - val_accuracy: 0.9716 - val_loss: 0.0761\n",
      "Epoch 15/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9609 - loss: 0.1109 - val_accuracy: 0.9672 - val_loss: 0.0902\n",
      "Epoch 16/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9620 - loss: 0.1082 - val_accuracy: 0.9725 - val_loss: 0.0759\n",
      "Epoch 17/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9632 - loss: 0.1044 - val_accuracy: 0.9724 - val_loss: 0.0772\n",
      "Epoch 18/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9638 - loss: 0.1012 - val_accuracy: 0.9760 - val_loss: 0.0683\n",
      "Epoch 19/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9665 - loss: 0.0951 - val_accuracy: 0.9743 - val_loss: 0.0737\n",
      "Epoch 20/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9657 - loss: 0.0974 - val_accuracy: 0.9739 - val_loss: 0.0713\n",
      "Epoch 21/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9660 - loss: 0.0944 - val_accuracy: 0.9683 - val_loss: 0.0798\n",
      "Epoch 22/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9661 - loss: 0.0945 - val_accuracy: 0.9781 - val_loss: 0.0634\n",
      "Epoch 23/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9671 - loss: 0.0910 - val_accuracy: 0.9762 - val_loss: 0.0616\n",
      "Epoch 24/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9692 - loss: 0.0857 - val_accuracy: 0.9724 - val_loss: 0.0725\n",
      "Epoch 25/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9691 - loss: 0.0879 - val_accuracy: 0.9760 - val_loss: 0.0622\n",
      "Epoch 26/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9689 - loss: 0.0839 - val_accuracy: 0.9826 - val_loss: 0.0531\n",
      "Epoch 27/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9704 - loss: 0.0839 - val_accuracy: 0.9804 - val_loss: 0.0541\n",
      "Epoch 28/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9710 - loss: 0.0801 - val_accuracy: 0.9796 - val_loss: 0.0566\n",
      "Epoch 29/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.0757 - val_accuracy: 0.9760 - val_loss: 0.0639\n",
      "Epoch 30/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9723 - loss: 0.0782 - val_accuracy: 0.9807 - val_loss: 0.0537\n",
      "Epoch 31/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9720 - loss: 0.0781 - val_accuracy: 0.9814 - val_loss: 0.0515\n",
      "Epoch 32/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9723 - loss: 0.0788 - val_accuracy: 0.9818 - val_loss: 0.0598\n",
      "Epoch 33/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9727 - loss: 0.0746 - val_accuracy: 0.9789 - val_loss: 0.0576\n",
      "Epoch 34/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9739 - loss: 0.0719 - val_accuracy: 0.9810 - val_loss: 0.0541\n",
      "Epoch 35/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9733 - loss: 0.0726 - val_accuracy: 0.9785 - val_loss: 0.0581\n",
      "Epoch 36/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9738 - loss: 0.0716 - val_accuracy: 0.9816 - val_loss: 0.0513\n",
      "Epoch 37/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9748 - loss: 0.0694 - val_accuracy: 0.9804 - val_loss: 0.0531\n",
      "Epoch 38/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9761 - loss: 0.0686 - val_accuracy: 0.9821 - val_loss: 0.0494\n",
      "Epoch 39/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9753 - loss: 0.0671 - val_accuracy: 0.9829 - val_loss: 0.0469\n",
      "Epoch 40/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9757 - loss: 0.0670 - val_accuracy: 0.9845 - val_loss: 0.0420\n",
      "Epoch 41/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9757 - loss: 0.0668 - val_accuracy: 0.9808 - val_loss: 0.0501\n",
      "Epoch 42/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9769 - loss: 0.0647 - val_accuracy: 0.9803 - val_loss: 0.0538\n",
      "Epoch 43/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9770 - loss: 0.0616 - val_accuracy: 0.9867 - val_loss: 0.0399\n",
      "Epoch 44/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9774 - loss: 0.0627 - val_accuracy: 0.9767 - val_loss: 0.0700\n",
      "Epoch 45/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9768 - loss: 0.0664 - val_accuracy: 0.9843 - val_loss: 0.0419\n",
      "Epoch 46/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9778 - loss: 0.0634 - val_accuracy: 0.9830 - val_loss: 0.0481\n",
      "Epoch 47/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9765 - loss: 0.0635 - val_accuracy: 0.9838 - val_loss: 0.0419\n",
      "Epoch 48/100\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9771 - loss: 0.0618 - val_accuracy: 0.9830 - val_loss: 0.0467\n",
      "\u001b[1m848/848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9863 - loss: 0.0411\n",
      "Test accuracy: 0.9858838319778442\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# filter for valid classes because the data is not clean\n",
    "valid_classes = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "mask_train = y_train.isin(valid_classes)\n",
    "mask_test = y_test.isin(valid_classes)\n",
    "\n",
    "# apply the mask to the training and testing data\n",
    "X_train_filtered = X_train_scaled[mask_train]\n",
    "y_train_filtered = y_train[mask_train]\n",
    "\n",
    "X_test_filtered = X_test_scaled[mask_test]\n",
    "y_test_filtered = y_test[mask_test]\n",
    "\n",
    "# one hot encode the target data\n",
    "y_train_categorical = to_categorical(y_train_filtered, num_classes=8)\n",
    "y_test_categorical = to_categorical(y_test_filtered, num_classes=8)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(128, activation='relu', input_shape=(X_train_filtered.shape[1],)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5)) \n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(8, activation='softmax'))  # Softmax for 8 output classes\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_filtered.shape[1], 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Compile the model with categorical crossentropy for multi-class classification\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model using the filtered training data\n",
    "model.fit(X_train_filtered, y_train_categorical, epochs=100, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the filtered test set\n",
    "test_loss, test_acc = model.evaluate(X_test_filtered, y_test_categorical)\n",
    "\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(MODEL_PATH)\n",
    "\n",
    "# Save the scaler and the column names to a pickle file\n",
    "with open(METADATA_PATH, 'wb') as f:\n",
    "    pickle.dump((scaler, X_train.columns), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001B281933880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001B281933880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "Predicted class: 4 - pointer_finger\n"
     ]
    }
   ],
   "source": [
    "# emg_data = [[2,-11,-11,-38,-26,-1,-3,-11,], [-2,4,14,26,45,66,19,25]]\n",
    "# emg_data = [[-4,2,2,1,-29,10,-1,-4], [4,-3,-4,9,6,2,2,-2]]\n",
    "emg_data = [[-7,-9,-14,-55,-117,-25,-11], [-8,8,8,20,50,98,9,12,5]]\n",
    "\n",
    "# emg_data = [[0, 0, 0, 0, 100, 0, 0, 0], [0, 0, 0, 0, 100, 0, 0, 0]]\n",
    "\n",
    "sensor1_data, sensor2_data = emg_data\n",
    "\n",
    "emg_features = np.concatenate((sensor1_data, sensor2_data))\n",
    "emg_features_df = pd.DataFrame([emg_features], columns=X_train.columns)\n",
    "\n",
    "emg_features_scaled = scaler.transform(emg_features_df)\n",
    "emg_features_reshaped = emg_features_scaled.reshape(1, -1)\n",
    "\n",
    "prediction = model.predict(emg_features_reshaped)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "# print(f\"Probabilities: {prediction}\")\n",
    "\n",
    "print(f\"Predicted class: {predicted_class} - {CLASSES[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Data preprocessing\u001b[39;00m\n\u001b[0;32m     14\u001b[0m emg_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((sensor1_data, sensor2_data))\n\u001b[1;32m---> 15\u001b[0m emg_features_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([emg_features], columns\u001b[38;5;241m=\u001b[39m\u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     17\u001b[0m emg_features_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(emg_features_df)\n\u001b[0;32m     18\u001b[0m emg_features_reshaped \u001b[38;5;241m=\u001b[39m emg_features_scaled\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example input data\n",
    "emg_data = [[-7, -9, -14, -55, -117, -25, -11], [-8, 8, 8, 20, 50, 98, 9, 12, 5]]\n",
    "\n",
    "sensor1_data, sensor2_data = emg_data\n",
    "\n",
    "# Start measuring time\n",
    "start_time = time.time()\n",
    "\n",
    "# Data preprocessing\n",
    "emg_features = np.concatenate((sensor1_data, sensor2_data))\n",
    "emg_features_df = pd.DataFrame([emg_features], columns=X_train.columns)\n",
    "\n",
    "emg_features_scaled = scaler.transform(emg_features_df)\n",
    "emg_features_reshaped = emg_features_scaled.reshape(1, -1)\n",
    "\n",
    "# Model prediction\n",
    "prediction = model.predict(emg_features_reshaped)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate processing time\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "# Output results\n",
    "print(f\"Predicted class: {predicted_class} - {CLASSES[predicted_class]}\")\n",
    "print(f\"Processing time: {processing_time:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
